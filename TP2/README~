LAST: 

  if (k < 4000)
          learningFactor = 0.05;
          kickthreshhold = 0.01;
  end
  if (k < 1500)
          learningFactor = 0.1;
          kickthreshhold = 0.05;
  end



------


TPE2 - MultiLayer Perceptron
=========

ITBA - 2015 

=========

WORKING 0-1
tp2(1,[1 25 15 1] , 1, 0.1, 0.3, 0.000001,1,125000,[0.02 0.1], 5, 0)
tp2(1,[1 35 15 1] , 1, 0.1, 0.9, 0.000001,1,125000,[0.02 0.1], 5, 0)
with 
x1 = [0 : 0.01 : 1];



QUICK LAUNCH :

tp2(5,[1 25 1] , 2, 0.1, 0.3, 0.000001,1,125000,[0.01 0.1], 5, [0.01])


Loading all the files into octave workspace (if it's not done automatically).



To train the perceptron you should call the octave function:
        - tp2(file, layerSize, beta, learningFactor, alpha, epsilon, activationFunction, 
                                iterationCount, adaptation, consistency, amount, errorRates)
    


REFERENCE 
=====


Parameters:
        + function (idx): FUNCTION INDEX . OURS IS 1. Test with another
        + layerSize (array): sizes of the perceptron layers -> [1 15 1]
        + beta (double): beta value for the activation function -> 1
        + learningFactor (double): parameter that steepens the learning curve -> 0.1
        + alpha (double): momentum variation parameter -> 0.9
        + epsilon (double): value to compare with the mean squared error -> 0.001
        + activationFunction (integer): activation function [values: 1=sigmoid, 2=exponential]
        + iterationCount (integer): number of iterations of the backpropagation algorithm being 0 infinite iterations.
        + adaptation (array): alpha and beta for etha adaptative improvement -> [0.01 0.1]
        + consistency (integer): consistency of the adaptative eta modification -> 10
        + errorRates (array): possible error values to compare with the outputs
Return values:
        - perceptron: Matrix that represents a trained perceptron (weights between neurons and layers).


       

Note: [It automatically generate statistics and a 3D Plot for the recently created and trained perceptron]



